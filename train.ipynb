{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import PIL\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import data\n",
    "from skimage.util import montage \n",
    "import skimage.transform as skTrans\n",
    "from skimage.transform import rotate, resize\n",
    "from PIL import Image, ImageOps  \n",
    "\n",
    "\n",
    "# neural imaging\n",
    "import nilearn as nl\n",
    "import nibabel as nib\n",
    "import nilearn.plotting as nlplt\n",
    "import gif_your_nifti.core as gif2nif # by Michael Notter (miykael)\n",
    "\n",
    "\n",
    "# ml libs\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import CSVLogger\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEGMENT_CLASSES = {\n",
    "    0 : 'NOT tumor',\n",
    "    1 : 'NECROTIC/CORE', \n",
    "    2 : 'EDEMA',\n",
    "    3 : 'ENHANCING' \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "VOLUME_SLICES = 100 \n",
    "VOLUME_START_AT = 22 \n",
    "\n",
    "TRAIN_DATASET_PATH = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/'\n",
    "VALIDATION_DATASET_PATH = '../input/brats20-dataset-training-validation/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dice loss as defined above for 4 classes\n",
    "def dice_coef(y_true, y_pred, smooth=1.0):\n",
    "    class_num = 4\n",
    "    for i in range(class_num):\n",
    "        y_true_f = K.flatten(y_true[:,:,:,i])\n",
    "        y_pred_f = K.flatten(y_pred[:,:,:,i])\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n",
    "        if i == 0:\n",
    "            total_loss = loss\n",
    "        else:\n",
    "            total_loss = total_loss + loss\n",
    "    total_loss = total_loss / class_num\n",
    "    return total_loss\n",
    "\n",
    "\n",
    " \n",
    "# define per class evaluation of dice coef\n",
    "def dice_coef_necrotic(y_true, y_pred, epsilon=1e-6):\n",
    "    intersection = K.sum(K.abs(y_true[:,:,:,1] * y_pred[:,:,:,1]))\n",
    "    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,1])) + K.sum(K.square(y_pred[:,:,:,1])) + epsilon)\n",
    "\n",
    "def dice_coef_edema(y_true, y_pred, epsilon=1e-6):\n",
    "    intersection = K.sum(K.abs(y_true[:,:,:,2] * y_pred[:,:,:,2]))\n",
    "    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,2])) + K.sum(K.square(y_pred[:,:,:,2])) + epsilon)\n",
    "\n",
    "def dice_coef_enhancing(y_true, y_pred, epsilon=1e-6):\n",
    "    intersection = K.sum(K.abs(y_true[:,:,:,3] * y_pred[:,:,:,3]))\n",
    "    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,3])) + K.sum(K.square(y_pred[:,:,:,3])) + epsilon)\n",
    "\n",
    "\n",
    "\n",
    "#Precision \n",
    "def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    \n",
    "#Sensitivity      \n",
    "def sensitivity(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "\n",
    "#Specificity \n",
    "def specificity(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from UNet import build_unet\n",
    "IMG_SIZE = 128\n",
    "input_layer = Input((IMG_SIZE, IMG_SIZE, 2))\n",
    "\n",
    "UNet_model = build_unet(input_layer, 'he_normal', 0.2)\n",
    "UNet_model.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    " metrics = ['accuracy',tf.keras.metrics.MeanIoU(num_classes=4), \n",
    " dice_coef, precision, sensitivity, specificity, dice_coef_necrotic, dice_coef_edema ,dice_coef_enhancing] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of directories with studies\n",
    "train_and_val_directories = [f.path for f in os.scandir(TRAIN_DATASET_PATH) if f.is_dir()]\n",
    "\n",
    "# file BraTS20_Training_355 has ill formatted name for for seg.nii file\n",
    "train_and_val_directories.remove(TRAIN_DATASET_PATH+'BraTS20_Training_355')\n",
    "\n",
    "\n",
    "def pathListIntoIds(dirList):\n",
    "    x = []\n",
    "    for i in range(0,len(dirList)):\n",
    "        x.append(dirList[i][dirList[i].rfind('/')+1:])\n",
    "    return x\n",
    "\n",
    "train_and_test_ids = pathListIntoIds(train_and_val_directories); \n",
    "\n",
    "    \n",
    "train_test_ids, val_ids = train_test_split(train_and_test_ids,test_size=0.2) \n",
    "train_ids, test_ids = train_test_split(train_test_ids,test_size=0.15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, dim=(IMG_SIZE,IMG_SIZE), batch_size = 1, n_channels = 2, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        Batch_ids = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(Batch_ids)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, Batch_ids):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, self.n_channels))\n",
    "        y = np.zeros((self.batch_size*VOLUME_SLICES, 240, 240))\n",
    "        Y = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, 4))\n",
    "\n",
    "        \n",
    "        # Generate data\n",
    "        for c, i in enumerate(Batch_ids):\n",
    "            case_path = os.path.join(TRAIN_DATASET_PATH, i)\n",
    "\n",
    "            data_path = os.path.join(case_path, f'{i}_flair.nii');\n",
    "            flair = nib.load(data_path).get_fdata()    \n",
    "\n",
    "            data_path = os.path.join(case_path, f'{i}_t1ce.nii');\n",
    "            ce = nib.load(data_path).get_fdata()\n",
    "            \n",
    "            data_path = os.path.join(case_path, f'{i}_seg.nii');\n",
    "            seg = nib.load(data_path).get_fdata()\n",
    "        \n",
    "            for j in range(VOLUME_SLICES):\n",
    "                 X[j +VOLUME_SLICES*c,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n",
    "                 X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n",
    "\n",
    "                 y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT];\n",
    "                    \n",
    "        # Generate masks\n",
    "        y[y==4] = 3;\n",
    "        mask = tf.one_hot(y, 4);\n",
    "        Y = tf.image.resize(mask, (IMG_SIZE, IMG_SIZE));\n",
    "        return X/np.max(X), Y\n",
    "\n",
    "\n",
    "# lists of directories with studies\n",
    "train_and_val_directories = [f.path for f in os.scandir(TRAIN_DATASET_PATH) if f.is_dir()]\n",
    "\n",
    "# file BraTS20_Training_355 has ill formatted name for for seg.nii file\n",
    "train_and_val_directories.remove(TRAIN_DATASET_PATH+'BraTS20_Training_355')\n",
    "\n",
    "\n",
    "def pathListIntoIds(dirList):\n",
    "    x = []\n",
    "    for i in range(0,len(dirList)):\n",
    "        x.append(dirList[i][dirList[i].rfind('/')+1:])\n",
    "    return x\n",
    "\n",
    "train_and_test_ids = pathListIntoIds(train_and_val_directories); \n",
    "\n",
    "    \n",
    "train_test_ids, val_ids = train_test_split(train_and_test_ids,test_size=0.2) \n",
    "train_ids, test_ids = train_test_split(train_test_ids,test_size=0.15) \n",
    "        \n",
    "training_generator = DataGenerator(train_ids)\n",
    "valid_generator = DataGenerator(val_ids)\n",
    "test_generator = DataGenerator(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "      keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=2, min_lr=0.000001, verbose=1),\n",
    " keras.callbacks.ModelCheckpoint(filepath = 'model_.{epoch:02d}-{val_loss:.6f}.m5',\n",
    "                            verbose=1, save_best_only=True, save_weights_only = True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "history =  model.fit(training_generator,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch=len(train_ids),\n",
    "                    callbacks= callbacks,\n",
    "                    validation_data = valid_generator\n",
    "                    )  \n",
    "model.save(\"final_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=history.history\n",
    "\n",
    "acc=hist['accuracy']\n",
    "val_acc=hist['val_accuracy']\n",
    "\n",
    "epoch=range(len(acc))\n",
    "\n",
    "loss=hist['loss']\n",
    "val_loss=hist['val_loss']\n",
    "\n",
    "train_dice=hist['dice_coef']\n",
    "val_dice=hist['val_dice_coef']\n",
    "\n",
    "f,ax=plt.subplots(1,4,figsize=(16,8))\n",
    "\n",
    "ax[0].plot(epoch,acc,'b',label='Training Accuracy')\n",
    "ax[0].plot(epoch,val_acc,'r',label='Validation Accuracy')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(epoch,loss,'b',label='Training Loss')\n",
    "ax[1].plot(epoch,val_loss,'r',label='Validation Loss')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(epoch,train_dice,'b',label='Training dice coef')\n",
    "ax[2].plot(epoch,val_dice,'r',label='Validation dice coef')\n",
    "ax[2].legend()\n",
    "\n",
    "ax[3].plot(epoch,hist['mean_io_u'],'b',label='Training mean IOU')\n",
    "ax[3].plot(epoch,hist['val_mean_io_u'],'r',label='Validation mean IOU')\n",
    "ax[3].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- rastislav 3d-mri-brain-tumor-segmentation-u-net on Kaggle"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
